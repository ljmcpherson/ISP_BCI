{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljmcpherson/ISP_BCI/blob/main/Another_copy_of_load_ECoG_motor_imagery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs6lxOn3_HiI"
      },
      "source": [
        "## Loading of Miller ECoG data of motor imagery\n",
        "\n",
        "includes some visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2_iGZZ6_HiK"
      },
      "outputs": [],
      "source": [
        "# @title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = 'motor_imagery.npz'\n",
        "url = \"https://osf.io/ksqv8/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqDQsVfE_HiM",
        "outputId": "2f2e8150-0738-413e-8db0-76fcf93596ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.9/179.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install packages (`nilearn`, `nimare`. `duecredit`), import `matplotlib` and set defaults\n",
        "# install packages to visualize brains and electrode locations\n",
        "!pip install nilearn --quiet\n",
        "!pip install nimare --quiet\n",
        "!pip install duecredit --quiet\n",
        "\n",
        "from matplotlib import rcParams\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] = 15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InsRnmvZ_HiP",
        "outputId": "a7a74015-9f05-47b3-f7dc-12c080300ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['t_off', 'stim_id', 't_on', 'srate', 'V', 'scale_uv', 'locs', 'hemisphere', 'lobe', 'gyrus', 'Brodmann_Area'])\n",
            "dict_keys(['t_off', 'stim_id', 't_on', 'srate', 'V', 'scale_uv', 'locs', 'hemisphere', 'lobe', 'gyrus', 'Brodmann_Area'])\n"
          ]
        }
      ],
      "source": [
        "# @title Data loading\n",
        "import numpy as np\n",
        "\n",
        "alldat = np.load(fname, allow_pickle=True)['dat']\n",
        "\n",
        "# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx.\n",
        "dat1 = alldat[1][0]\n",
        "dat2 = alldat[1][1]\n",
        "\n",
        "print(dat1.keys())\n",
        "print(dat2.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5tAFwbiemEuJ",
        "outputId": "d78af042-5436-4f4d-919e-688af8c74f12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(390680, 64)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat1[\"V\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C8aZS7qWmZaB",
        "outputId": "7eb1ea40-8438-44c2-9f38-a0936dde48a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000, 4)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a = np.arange(46)\n",
        "a = [1,2,3,4]\n",
        "dat1[\"V\"][1000:2000][:,a].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRe7s5ZbhHa5",
        "outputId": "ae8f6788-5cfa-4072-d573-9bf0df636b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
            " 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
            " 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
            " 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
            " 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            " 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1.\n",
            " 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n",
            " 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
            " 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "alldat = np.load(fname, allow_pickle=True)['dat']\n",
        "channels = np.arange(46) # channels from 0 to 46 are picked / you can change this to pick only some channels  - numpy.array\n",
        "\n",
        "def split_classes(data, stim_id_1, stim_id_2, sub = 0, binary_stim_id = True , channels = channels):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - data: Dictionary containing the data.\n",
        "    - stim_id_1: Stimulus ID for class 1.\n",
        "    - stim_id_2: Stimulus ID for class 2.\n",
        "    - sub : constraint on the t_off --> 3000 - sub\n",
        "\n",
        "    Returns:\n",
        "    - class_1_data: Data dictionary for class 1.\n",
        "    - class_2_data: Data dictionary for class 2.\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries for the two classes\n",
        "    class_1_data = {'V': [], 't_on': [], 't_off': [], 'stim_id': []}\n",
        "    class_2_data = {'V': [], 't_on': [], 't_off': [], 'stim_id': []}\n",
        "    # class_1_data = {'V': [], 't_on': []}\n",
        "    # class_2_data = {'V': [], 't_on': []}\n",
        "\n",
        "    # Iterate through the stimulus IDs and split the data\n",
        "    for i, stim_id in enumerate(data['stim_id']):\n",
        "        t_on = data['t_on'][i]\n",
        "        t_off = data['t_off'][i] - sub\n",
        "        if stim_id == stim_id_1:\n",
        "            class_1_data['V'].append(data['V'][t_on:t_off][:,channels])\n",
        "            class_1_data['t_on'].append(t_on)\n",
        "            class_1_data['t_off'].append(t_off)\n",
        "            class_1_data['stim_id'].append(stim_id)\n",
        "        elif stim_id == stim_id_2:\n",
        "            class_2_data['V'].append(data['V'][t_on:t_off][:,channels])\n",
        "            class_2_data['t_on'].append(t_on)\n",
        "            class_2_data['t_off'].append(t_off)\n",
        "            class_2_data['stim_id'].append(stim_id)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    class_1_data['V'] = np.array(class_1_data['V'])\n",
        "    class_1_data['t_on'] = np.array(class_1_data['t_on'])\n",
        "    class_1_data['t_off'] = np.array(class_1_data['t_off'])\n",
        "    class_1_data['stim_id'] = np.array(class_1_data['stim_id'])\n",
        "\n",
        "    class_2_data['V'] = np.array(class_2_data['V'])\n",
        "    class_2_data['t_on'] = np.array(class_2_data['t_on'])\n",
        "    class_2_data['t_off'] = np.array(class_2_data['t_off'])\n",
        "    class_2_data['stim_id'] = np.array(class_2_data['stim_id'])\n",
        "\n",
        "\n",
        "    if binary_stim_id:\n",
        "        class_1_data['stim_id'] = 1 - (class_1_data['stim_id']==11).astype(int)\n",
        "        class_2_data['stim_id'] = (class_2_data['stim_id']==12).astype(int)\n",
        "\n",
        "    return class_1_data, class_2_data\n",
        "\n",
        "\n",
        "def get_all(data, stim_id_1 = 11, stim_id_2 = 12, sub = 0, binary_stim_id = True, channels = channels):\n",
        "  \"\"\"\n",
        "  //TODO\n",
        "  \"\"\"\n",
        "  real = {\"tongue\" : [], \"hand\" : []}\n",
        "  imagine = {\"tongue\" : [], \"hand\" : []}\n",
        "  # Iterate over the datasets\n",
        "  for i in range(7):\n",
        "      # Split classes for the current dataset\n",
        "      real_classes_0, real_classes_1 = split_classes(alldat[i][0], stim_id_1, stim_id_2, sub = sub, binary_stim_id = binary_stim_id, channels = channels)\n",
        "      imagine_classes_0,  imagine_classes_1 = split_classes(alldat[i][1], stim_id_1, stim_id_2, sub = sub, binary_stim_id = binary_stim_id, channels = channels)\n",
        "\n",
        "      # Append the results to the lists\n",
        "      real[\"tongue\"].append(real_classes_0)\n",
        "      imagine[\"tongue\"].append(imagine_classes_0)\n",
        "      real[\"hand\"].append(real_classes_1)\n",
        "      imagine[\"hand\"].append(imagine_classes_1)\n",
        "\n",
        "\n",
        "  real[\"tongue\"] = np.array(real[\"tongue\"])\n",
        "  imagine[\"tongue\"] = np.array(imagine[\"tongue\"])\n",
        "  real[\"hand\"] = np.array(real[\"hand\"])\n",
        "  imagine[\"hand\"] = np.array(imagine[\"hand\"])\n",
        "\n",
        "  return real, imagine\n",
        "# split '4' = rt, rh, it, ih\n",
        "# split '2' = rt & it, rh & ih\n",
        "# split 'r' = rt, rh\n",
        "# split 'i' = it, ih\n",
        "def get_X_Y(real, imagine, channels = channels, flatten = True, shuffle = True, split = 'r'):\n",
        "    \"\"\"\n",
        "    //TODO\n",
        "\n",
        "    Didnt need to use the stim_id from dictionaries ... Might use them later ...\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(real['tongue'].shape[0]):\n",
        "        if split == '4':\n",
        "          X.append(real['tongue'][i]['V'])\n",
        "          X.append(imagine['tongue'][i]['V'])\n",
        "          Y.append(np.zeros(real['tongue'][i]['V'].shape[0]))\n",
        "          Y.append(np.zeros(imagine['tongue'][i]['V'].shape[0]) + 1)\n",
        "        elif split == '2':\n",
        "          X.append(real['tongue'][i]['V'])\n",
        "          X.append(imagine['tongue'][i]['V'])\n",
        "          Y.append(np.zeros(real['tongue'][i]['V'].shape[0]))\n",
        "          Y.append(np.zeros(imagine['tongue'][i]['V'].shape[0]))\n",
        "        elif split == 'r':\n",
        "          X.append(real['tongue'][i]['V'])\n",
        "          Y.append(np.zeros(real['tongue'][i]['V'].shape[0]))\n",
        "        elif split == 'i':\n",
        "          X.append(imagine['tongue'][i]['V'])\n",
        "          Y.append(np.zeros(imagine['tongue'][i]['V'].shape[0]))\n",
        "\n",
        "    for i in range(real['hand'].shape[0]):\n",
        "        if split == '4':\n",
        "          X.append(real['hand'][i]['V'])\n",
        "          X.append(imagine['hand'][i]['V'])\n",
        "          Y.append(np.ones(real['hand'][i]['V'].shape[0]) + 1)\n",
        "          Y.append(np.ones(imagine['hand'][i]['V'].shape[0]) + 2)\n",
        "        elif split == '2':\n",
        "          X.append(real['hand'][i]['V'])\n",
        "          X.append(imagine['hand'][i]['V'])\n",
        "          Y.append(np.ones(real['hand'][i]['V'].shape[0]))\n",
        "          Y.append(np.ones(imagine['hand'][i]['V'].shape[0]))\n",
        "        elif split == 'r':\n",
        "          X.append(real['hand'][i]['V'])\n",
        "          Y.append(np.ones(real['hand'][i]['V'].shape[0]))\n",
        "        elif split == 'i':\n",
        "          X.append(imagine['hand'][i]['V'])\n",
        "          Y.append(np.ones(imagine['hand'][i]['V'].shape[0]))\n",
        "\n",
        "    X = np.array(X)\n",
        "    X = X.reshape(-1, real['tongue'][i]['V'].shape[1], channels.shape[0]).transpose(0, 2, 1)\n",
        "\n",
        "    Y = np.array(Y)\n",
        "    Y = Y.reshape(-1)\n",
        "\n",
        "\n",
        "    if flatten:\n",
        "      X = X.reshape(-1, X.shape[1]*X.shape[2])\n",
        "\n",
        "    if shuffle:\n",
        "      permutation = np.random.permutation(X.shape[0])\n",
        "      X = X[permutation]\n",
        "      Y = Y[permutation]\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "\n",
        "real, imagine = get_all(data=alldat, sub = 2000)\n",
        "X,Y = get_X_Y(real, imagine)\n",
        "print(Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVkBxH_xlxs5"
      },
      "source": [
        "# possible issue ? different number of channels ? Temp solution by setting 46 hard constraint...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZqEswdEl5YJ",
        "outputId": "d86233a1-ddf1-481a-f972-648e2bf44a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject 1 - V - tongue - real movement: (30, 2000, 46)\n",
            "Subject 1 - t_on - tongue - real movement: [ 28240  52440  58520  76680  82680  88720 100760 106800 112840 131040\n",
            " 137080 149160 155160 179320 185320 197400 227560 239640 257720 263760\n",
            " 275840 281840 287880 293880 299880 330080 342200 348240 360360 366440]\n",
            "Subject 1 - t_off - tongue - real movement: [ 30240  54440  60520  78680  84680  90720 102760 108800 114840 133040\n",
            " 139080 151160 157160 181320 187320 199400 229560 241640 259720 265760\n",
            " 277840 283840 289880 295880 301880 332080 344200 350240 362360 368440]\n",
            "Subject 1 - stim_id - tongue - real movement: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "real, imagine = get_all(data=alldat, sub = 1000)\n",
        "\n",
        "subject_id = 0\n",
        "exp = real\n",
        "action = \"tongue\"\n",
        "\n",
        "print(\"Subject 1 - V - tongue - real movement:\", exp[action][subject_id]['V'].shape)\n",
        "print(\"Subject 1 - t_on - tongue - real movement:\", exp[action][subject_id]['t_on'])\n",
        "print(\"Subject 1 - t_off - tongue - real movement:\", exp[action][subject_id]['t_off'])\n",
        "print(\"Subject 1 - stim_id - tongue - real movement:\", exp[action][subject_id]['stim_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hgKTJEVXKnT",
        "outputId": "a744e9e1-0bbf-45e3-bb40-24a8be00d4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(420, 92000)\n",
            "(420,)\n",
            "[0. 1.]\n",
            "(420, 92000)\n",
            "(420,)\n",
            "[0. 1.]\n",
            "[[-0.2158  -0.1781  -0.1556 ]\n",
            " [-2.186   -2.273   -2.35   ]\n",
            " [ 0.3286  -0.428   -0.129  ]\n",
            " [ 0.3833   0.332    0.36   ]\n",
            " [-0.06976  0.0889   0.08984]]\n",
            "[1. 0. 1. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "X,Y = get_X_Y(real, imagine, split = 'r')\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "print(np.unique(Y))\n",
        "# print(X[:5,:3]) # first 5 samples and their first 3 features\n",
        "# print(Y[:5]) # first 5 target values\n",
        "\n",
        "X,Y = get_X_Y(real, imagine, split = 'r')\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "print(np.unique(Y))\n",
        "print(X[:5,:3]) # first 5 samples and their first 3 features\n",
        "print(Y[:5]) # first 5 target values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrKCag1v6uK5",
        "outputId": "543060de-9faf-4326-e681-d58be28921bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.4.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
            "Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: lazypredict\n",
            "Successfully installed lazypredict-0.2.12\n"
          ]
        }
      ],
      "source": [
        "!pip install lazypredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EhDjgA1W-od"
      },
      "outputs": [],
      "source": [
        "#Feature extraction function\n",
        "def get_features_single(signal, sample_rate=1000, plot=False, log_scale=True):\n",
        "\n",
        "    f_orig, psd_orig = compute_psd(signal, sample_rate, log_scale=log_scale)\n",
        "\n",
        "    f_filt_LFB, psd_filt_LFB = filter_psd(f_orig, psd_orig, low_cut=8, high_cut=32)\n",
        "    f_filt_HFB, psd_filt_HFB = filter_psd(f_orig, psd_orig, low_cut=76, high_cut=100)\n",
        "\n",
        "    def extract_features(psd_values):\n",
        "        return {\n",
        "            'mean': np.mean(psd_values),\n",
        "            'max': np.max(psd_values),\n",
        "            'std': np.std(psd_values),\n",
        "            'min': np.min(psd_values),\n",
        "            'raw_values': psd_values\n",
        "        }\n",
        "\n",
        "    features_LFB = extract_features(psd_filt_LFB)\n",
        "    features_HFB = extract_features(psd_filt_HFB)\n",
        "\n",
        "    # Plotting\n",
        "    if plot:\n",
        "        t = np.arange(len(signal)) / sample_rate\n",
        "\n",
        "        plt.figure(figsize=(16, 9))\n",
        "\n",
        "        # Plot the original signal\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(t, signal, label='Original Signal')\n",
        "        plt.title(f'Original Signal')\n",
        "        plt.xlabel('Time [s]')\n",
        "        plt.ylabel('Amplitude')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot the PSD with vertical lines for the filtered frequency bands\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(f_orig, psd_orig, label='PSD of Original Signal')\n",
        "        filt_values = [8, 32, 76, 100]\n",
        "        for x in filt_values:\n",
        "            plt.vlines(x, ymin=min(psd_orig), ymax=max(psd_orig), colors='r', linestyles='--')\n",
        "        plt.title('Power Spectral Density of Original Signal')\n",
        "        plt.xlabel('Frequency [Hz]')\n",
        "        plt.ylabel('Log Power')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return features_LFB, features_HFB\n",
        "\n",
        "def get_features_all(X, sample_rate=1000, type_feature='statistical', out_features='low+high'):\n",
        "    feature_list = []\n",
        "\n",
        "    for sample in X:\n",
        "        S = []\n",
        "        for signal in sample:\n",
        "            features_LFB, features_HFB = get_features_single(\n",
        "                signal,\n",
        "                sample_rate=sample_rate,\n",
        "                plot=False,\n",
        "                log_scale=True\n",
        "            )\n",
        "\n",
        "            # Select feature type based on 'type_feature'\n",
        "            if type_feature == 'statistical':\n",
        "                selected_LFB = [features_LFB['mean'], features_LFB['std']]\n",
        "                selected_HFB = [features_HFB['mean'], features_HFB['std']]\n",
        "            elif type_feature == 'raw':\n",
        "                selected_LFB = features_LFB['raw_values']\n",
        "                selected_HFB = features_HFB['raw_values']\n",
        "            elif type_feature == 'all':\n",
        "                selected_LFB = [features_LFB['mean'], features_LFB['std'], features_LFB['max'], features_LFB['min']]\n",
        "                selected_HFB = [features_HFB['mean'], features_HFB['std'], features_HFB['max'], features_HFB['min']]\n",
        "\n",
        "\n",
        "            # Combine features based on 'out_features'\n",
        "            if out_features == 'low':\n",
        "                S.append(selected_LFB)\n",
        "            elif out_features == 'high':\n",
        "                S.append(selected_HFB)\n",
        "            elif out_features == 'low+high':\n",
        "                combined_features = np.array([selected_LFB, selected_HFB]).flatten()\n",
        "                S.append(combined_features)\n",
        "\n",
        "        feature_list.append(S)\n",
        "\n",
        "    return np.array(feature_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47hH38e_4wj2"
      },
      "source": [
        "Phase and magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Af93KzoGRd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.fft import fft, fftfreq\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the compute_psd function\n",
        "def compute_psd(signal, sample_rate, log_scale=True):\n",
        "    freqs = np.fft.fftfreq(len(signal), 1/sample_rate)\n",
        "    psd = np.abs(np.fft.fft(signal))**2\n",
        "    if log_scale:\n",
        "        psd = np.log(psd + 1e-8)\n",
        "    return freqs, psd\n",
        "\n",
        "# Define the get_features_single function\n",
        "def get_features_single(signal, sample_rate=1000, log_scale=True):\n",
        "    f_orig, psd_orig = compute_psd(signal, sample_rate, log_scale=log_scale)\n",
        "\n",
        "    # Filter and get PSD for low frequency band (example)\n",
        "    low_cut, high_cut = 8, 32\n",
        "    psd_filt_LFB = psd_orig[(f_orig >= low_cut) & (f_orig <= high_cut)]\n",
        "\n",
        "    # Filter and get PSD for high frequency band (example)\n",
        "    low_cut, high_cut = 32, 100\n",
        "    psd_filt_HFB = psd_orig[(f_orig >= low_cut) & (f_orig <= high_cut)]\n",
        "\n",
        "    # Combine features from different bands (example)\n",
        "    features = np.concatenate([psd_filt_LFB, psd_filt_HFB])\n",
        "    return features\n",
        "\n",
        "# Define the get_features_all function\n",
        "def get_features_all(data, sample_rate=1000):\n",
        "    num_samples, num_points = data.shape\n",
        "    features_list = []\n",
        "\n",
        "    for sample in data:\n",
        "        features = get_features_single(sample, sample_rate=sample_rate)\n",
        "        features_list.append(features)\n",
        "\n",
        "    return np.array(features_list)\n",
        "\n",
        "# Function to split data into relevant frequency bands and compute PSD and phase features\n",
        "def compute_band_features(data, sampling_rate, bands):\n",
        "    num_samples, num_features = data.shape\n",
        "    num_bands = len(bands)\n",
        "    band_features = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sample_features = []\n",
        "        for (low_freq, high_freq) in bands:\n",
        "            yf = fft(data[i])\n",
        "            xf = fftfreq(num_features, 1 / sampling_rate)\n",
        "            band_indices = np.where((xf >= low_freq) & (xf <= high_freq))[0]\n",
        "\n",
        "            psd_band = np.abs(yf[band_indices]) ** 2\n",
        "            phase_band = np.angle(yf[band_indices])\n",
        "\n",
        "            sample_features.extend(np.concatenate((psd_band, phase_band)))\n",
        "        band_features.append(sample_features)\n",
        "\n",
        "    return np.array(band_features)\n",
        "\n",
        "# Define frequency bands (example bands: Delta, Theta, Alpha, Beta, Gamma)\n",
        "bands = [(0.5, 4), (4, 8), (8, 12), (12, 30), (30, 50)]\n",
        "\n",
        "# Separate both X and Y as test and train data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2024)\n",
        "\n",
        "# Get features for train and test\n",
        "X_train_features = get_features_all(X_train, sample_rate=1000)\n",
        "X_test_features = get_features_all(X_test, sample_rate=1000)\n",
        "\n",
        "# Compute band-specific features\n",
        "sampling_rate = 1000  # Adjust according to your data's sampling rate\n",
        "band_features_train = compute_band_features(X_train_features, sampling_rate, bands)\n",
        "band_features_test = compute_band_features(X_test_features, sampling_rate, bands)\n",
        "\n",
        "# Standardize band features\n",
        "scaler_band = StandardScaler()\n",
        "band_features_train = scaler_band.fit_transform(band_features_train)\n",
        "band_features_test = scaler_band.transform(band_features_test)\n",
        "\n",
        "# Combine original and band-specific features\n",
        "X_train_combined = np.hstack((X_train_features, band_features_train))\n",
        "X_test_combined = np.hstack((X_test_features, band_features_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0vA-4zZVGnI"
      },
      "outputs": [],
      "source": [
        "# @title Random Forest Classifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYEgS8FZVHwM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gPKgTSWVUnJ",
        "outputId": "b3f15b4b-82dd-46b5-9a7b-2aefcb61d0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy score with 1000 decision-trees : 0.4762\n"
          ]
        }
      ],
      "source": [
        "#Random Forest\n",
        "# Import Random Forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate the classifier\n",
        "rfc = RandomForestClassifier(n_estimators= 1000, random_state=0)\n",
        "\n",
        "# fit the model\n",
        "rfc.fit(X_train_combined, Y_train)\n",
        "\n",
        "# Predict the Test set results\n",
        "y_pred = rfc.predict(X_test_combined)\n",
        "\n",
        "# Check accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Model accuracy score with 1000 decision-trees : {0:0.4f}'. format(accuracy_score(Y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6wxcNuzKf5T"
      },
      "source": [
        "Change for Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVV__DTNDKMt",
        "outputId": "9d47330a-b8ac-4e38-9163-cb1a93681b4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 28/29 [07:05<00:39, 39.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 145, number of negative: 149\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.860149 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4450123\n",
            "[LightGBM] [Info] Number of data points in the train set: 294, number of used features: 45000\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493197 -> initscore=-0.027213\n",
            "[LightGBM] [Info] Start training from score -0.027213\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [08:45<00:00, 18.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
            "Model                                                                           \n",
            "KNeighborsClassifier               0.57               0.57     0.57      0.57   \n",
            "BaggingClassifier                  0.56               0.57     0.57      0.56   \n",
            "AdaBoostClassifier                 0.56               0.55     0.55      0.55   \n",
            "LogisticRegression                 0.53               0.53     0.53      0.53   \n",
            "DecisionTreeClassifier             0.52               0.52     0.52      0.52   \n",
            "CalibratedClassifierCV             0.52               0.52     0.52      0.51   \n",
            "PassiveAggressiveClassifier        0.52               0.52     0.52      0.52   \n",
            "RidgeClassifierCV                  0.51               0.51     0.51      0.51   \n",
            "RidgeClassifier                    0.51               0.51     0.51      0.51   \n",
            "NuSVC                              0.51               0.51     0.51      0.51   \n",
            "DummyClassifier                    0.48               0.50     0.50      0.32   \n",
            "LabelPropagation                   0.48               0.50     0.50      0.32   \n",
            "LabelSpreading                     0.48               0.50     0.50      0.32   \n",
            "QuadraticDiscriminantAnalysis      0.49               0.49     0.49      0.49   \n",
            "ExtraTreesClassifier               0.49               0.49     0.49      0.49   \n",
            "LinearDiscriminantAnalysis         0.49               0.49     0.49      0.49   \n",
            "RandomForestClassifier             0.49               0.49     0.49      0.49   \n",
            "BernoulliNB                        0.49               0.49     0.49      0.48   \n",
            "SGDClassifier                      0.48               0.48     0.48      0.48   \n",
            "LinearSVC                          0.48               0.48     0.48      0.48   \n",
            "Perceptron                         0.48               0.48     0.48      0.48   \n",
            "SVC                                0.48               0.48     0.48      0.48   \n",
            "XGBClassifier                      0.48               0.47     0.47      0.48   \n",
            "NearestCentroid                    0.44               0.44     0.44      0.42   \n",
            "LGBMClassifier                     0.44               0.44     0.44      0.44   \n",
            "GaussianNB                         0.43               0.42     0.42      0.37   \n",
            "ExtraTreeClassifier                0.40               0.40     0.40      0.40   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "KNeighborsClassifier                 2.15  \n",
            "BaggingClassifier                   50.00  \n",
            "AdaBoostClassifier                  97.25  \n",
            "LogisticRegression                   4.66  \n",
            "DecisionTreeClassifier               9.98  \n",
            "CalibratedClassifierCV              11.56  \n",
            "PassiveAggressiveClassifier          2.21  \n",
            "RidgeClassifierCV                    2.12  \n",
            "RidgeClassifier                      3.25  \n",
            "NuSVC                                8.08  \n",
            "DummyClassifier                      1.33  \n",
            "LabelPropagation                     2.60  \n",
            "LabelSpreading                       2.64  \n",
            "QuadraticDiscriminantAnalysis       10.23  \n",
            "ExtraTreesClassifier                 3.39  \n",
            "LinearDiscriminantAnalysis          10.92  \n",
            "RandomForestClassifier              11.25  \n",
            "BernoulliNB                          2.05  \n",
            "SGDClassifier                        1.78  \n",
            "LinearSVC                           11.83  \n",
            "Perceptron                           1.78  \n",
            "SVC                                  9.11  \n",
            "XGBClassifier                      159.41  \n",
            "NearestCentroid                      1.43  \n",
            "LGBMClassifier                      99.72  \n",
            "GaussianNB                           1.83  \n",
            "ExtraTreeClassifier                  1.34  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "clf = LazyClassifier(predictions=True)\n",
        "models, predictions = clf.fit(X_train_combined, X_test_combined, y_train, y_test)\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUK-_H8stNMh"
      },
      "source": [
        "# Dimension reduction ---> need more analysis to find the best number of components. etc .. other methods maybe ....\n",
        "\n",
        "\n",
        "This might take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpwtekcLgL86"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_components = 30\n",
        "pca = PCA(n_components=n_components)\n",
        "new_X = pca.fit_transform(X)\n",
        "\n",
        "\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_X, Y, test_size=0.2, random_state=1234) # 80 percent of data as train 20 percent test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsJRpGkDtrga"
      },
      "source": [
        "# make a model and train and get the accuracy... You can see it overfits. ... maybe another method . randomforset ???? better feature selection/extraction ? run pca then t-sne ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXer-qwItmfI",
        "outputId": "c254de8c-e24f-49e4-a477-2d484db6a7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.8363095238095238\n",
            "Test Accuracy: 0.4880952380952381\n"
          ]
        }
      ],
      "source": [
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = svc.predict(X_train)\n",
        "y_test_pred = svc.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Train Accuracy: {train_accuracy}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1ZDFSs9t_Q5"
      },
      "source": [
        "# :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wt_kIBEQZuu"
      },
      "source": [
        "Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9knOSwVOqbO",
        "outputId": "bdb7c15e-e0d4-4ef2-c5fd-b448407bfa7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5119047619047619\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=20)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# SMOTE for data augmentation\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Model training with hyperparameter tuning\n",
        "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
        "grid = GridSearchCV(RandomForestClassifier(), param_grid, refit=True, cv=5)\n",
        "grid.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng01VoorQJEy"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q--HOiMkqu5Q"
      },
      "outputs": [],
      "source": [
        "real, imagine = get_all(data=alldat, sub = 0)\n",
        "X,Y = get_X_Y(real, imagine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChW_6q61uW2G",
        "outputId": "5d61cc30-2ada-416a-d3bf-e363f62f4aa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(420, 138000)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ooUZcrkzKNv",
        "outputId": "5334a89f-6c12-443f-c67a-b84679da4eeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(Y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "e6felYJLwxBz",
        "outputId": "0b284d9b-c370-40cb-a74e-5218fcb8618d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'l2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-db25f8aa7bdc>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Build the Bidirectional GRU model with Batch Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'l2' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X and Y are already defined\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "time_steps = 10  # Adjust the time steps as per your requirement\n",
        "n_features = X_train.shape[1] // time_steps\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, n_features))\n",
        "\n",
        "# Build the Bidirectional GRU model with Batch Normalization\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=(time_steps, n_features)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=False, kernel_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with RMSprop optimizer\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_bidirectional_gru_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqHGKuNKwKkp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X and Y are already defined\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "time_steps = 10  # Adjust the time steps as per your requirement\n",
        "n_features = X_train.shape[1] // time_steps\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, n_features))\n",
        "\n",
        "# Build the LSTM model with more regularization\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(time_steps, n_features), return_sequences=True, kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OfhUuQNuagh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X and Y are already defined\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "time_steps = 10  # Adjust the time steps as per your requirement\n",
        "n_features = X_train.shape[1] // time_steps\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, n_features))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(time_steps, n_features), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfsfxcjYxWvL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X and Y are already defined\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "# pca = PCA(n_components=1000)  # Adjust n_components as needed\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "time_steps = 5  # Adjust the time steps as per your requirement\n",
        "n_features = X_train.shape[1] // time_steps\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, n_features))\n",
        "\n",
        "# Build the simplified Bidirectional GRU model with Batch Normalization\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(GRU(32, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=(time_steps, n_features)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Bidirectional(GRU(32, return_sequences=False, kernel_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adam optimizer and learning rate scheduler\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping, learning rate reduction, and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "checkpoint = ModelCheckpoint('best_simplified_gru_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cmayi2A1GCl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X and Y are already defined\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction with PCA\n",
        "# pca = PCA(n_components=1000)  # Adjust n_components as needed\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "time_steps = 5  # Adjust the time steps as per your requirement\n",
        "n_features = X_train.shape[1] // time_steps\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, n_features))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, n_features))\n",
        "\n",
        "# Build the simplified Bidirectional GRU model with Batch Normalization\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(GRU(32, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=(time_steps, n_features)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Bidirectional(GRU(32, return_sequences=False, kernel_regularizer=l2(0.01))))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with Adam optimizer and learning rate scheduler\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping, learning rate reduction, and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "checkpoint = ModelCheckpoint('best_simplified_gru_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlfi0VA0rXOJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Assuming new_X and Y are already defined\n",
        "\n",
        "# Shuffle your dataset to ensure varied model training\n",
        "X, Y = shuffle(X, Y, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Optional: PCA for dimensionality reduction\n",
        "# pca = PCA(n_components=20)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "# Assuming the data has a time step dimension. If not, you need to introduce it.\n",
        "time_steps = 1  # You can adjust the time steps as per your requirement\n",
        "X_train = X_train.reshape((X_train.shape[0], time_steps, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], time_steps, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(time_steps, X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtEPzBRosySv"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwkhn6g7Px31"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Shuffle your dataset to ensure varied model training\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=1234)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Shuffle your dataset to ensure varied model training\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Optional: PCA for dimensionality reduction\n",
        "pca = PCA(n_components=20)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Step 2: Build the Neural Network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "\n",
        "# Step 3: Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Step 4: Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWE5V89oUihH"
      },
      "source": [
        "Added fourier transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Y7ECjnbdUHTf",
        "outputId": "cb6bf3d1-c59b-45d2-eb76-0e14773ab1b8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'new_X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c884a270c193>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Feature scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_X' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.fft import fft\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to compute Fourier Transform and extract phase features\n",
        "def compute_fourier_features(data, sampling_rate):\n",
        "    \"\"\"\n",
        "    Compute Fourier Transform and extract phase features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Input data (2D array where rows are samples and columns are features)\n",
        "    - sampling_rate: Sampling rate of the data\n",
        "\n",
        "    Returns:\n",
        "    - phase_features: Extracted phase features\n",
        "    \"\"\"\n",
        "    num_samples, num_features = data.shape\n",
        "    phase_features = np.zeros_like(data, dtype=np.float32)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Perform Fourier Transform\n",
        "        yf = fft(data[i])\n",
        "\n",
        "        # Extract phase information\n",
        "        phase = np.angle(yf)\n",
        "\n",
        "        # Use phase information as features\n",
        "        phase_features[i] = phase\n",
        "\n",
        "    return phase_features\n",
        "\n",
        "# Example data preparation (replace with your actual data)\n",
        "# X, Y = your_data_loading_function()\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Optional: PCA for dimensionality reduction (if needed)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=20)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Compute Fourier features\n",
        "sampling_rate = 1  # Adjust according to your data\n",
        "phase_features_train = compute_fourier_features(X_train, sampling_rate)\n",
        "phase_features_test = compute_fourier_features(X_test, sampling_rate)\n",
        "\n",
        "# Standardize phase features\n",
        "scaler_phase = StandardScaler()\n",
        "phase_features_train = scaler_phase.fit_transform(phase_features_train)\n",
        "phase_features_test = scaler_phase.transform(phase_features_test)\n",
        "\n",
        "# Combine original and phase features\n",
        "X_train_combined = np.hstack((X_train, phase_features_train))\n",
        "X_test_combined = np.hstack((X_test, phase_features_test))\n",
        "\n",
        "# Build the Neural Network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_combined.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train_combined, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test_combined)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QIDdfp9I1K4F",
        "outputId": "3c67eb6c-7621-4edf-c6ee-da7045084e37"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'new_X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d1422503c93c>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmagnitude_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Feature scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_X' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.fft import fft\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Function to compute Fourier Transform and extract magnitude and phase features\n",
        "def compute_fourier_features(data, sampling_rate):\n",
        "    \"\"\"\n",
        "    Compute Fourier Transform and extract magnitude and phase features.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Input data (2D array where rows are samples and columns are features)\n",
        "    - sampling_rate: Sampling rate of the data\n",
        "\n",
        "    Returns:\n",
        "    - magnitude_features: Extracted magnitude features\n",
        "    - phase_features: Extracted phase features\n",
        "    \"\"\"\n",
        "    num_samples, num_features = data.shape\n",
        "    magnitude_features = np.zeros_like(data, dtype=np.float32)\n",
        "    phase_features = np.zeros_like(data, dtype=np.float32)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Perform Fourier Transform\n",
        "        yf = fft(data[i])\n",
        "\n",
        "        # Extract magnitude and phase information\n",
        "        magnitude = np.abs(yf)\n",
        "        phase = np.angle(yf)\n",
        "\n",
        "        # Use magnitude and phase information as features\n",
        "        magnitude_features[i] = magnitude\n",
        "        phase_features[i] = phase\n",
        "\n",
        "    return magnitude_features, phase_features\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_X, Y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Compute Fourier features\n",
        "sampling_rate = 1  # Adjust according to your data\n",
        "magnitude_features_train, phase_features_train = compute_fourier_features(X_train, sampling_rate)\n",
        "magnitude_features_test, phase_features_test = compute_fourier_features(X_test, sampling_rate)\n",
        "\n",
        "# Standardize magnitude and phase features\n",
        "scaler_magnitude = StandardScaler()\n",
        "magnitude_features_train = scaler_magnitude.fit_transform(magnitude_features_train)\n",
        "magnitude_features_test = scaler_magnitude.transform(magnitude_features_test)\n",
        "\n",
        "scaler_phase = StandardScaler()\n",
        "phase_features_train = scaler_phase.fit_transform(phase_features_train)\n",
        "phase_features_test = scaler_phase.transform(phase_features_test)\n",
        "\n",
        "# Combine original, magnitude, and phase features\n",
        "X_train_combined = np.hstack((X_train, magnitude_features_train, phase_features_train))\n",
        "X_test_combined = np.hstack((X_test, magnitude_features_test, phase_features_test))\n",
        "\n",
        "# Build the Neural Network\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_combined.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(X_train_combined, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test_combined, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make Predictions\n",
        "y_pred = model.predict(X_test_combined)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f'Final Test Accuracy: {final_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSLp7lAVv32i"
      },
      "source": [
        "#1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHFZYEoLFSYT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_classes(data, stim_id_1, stim_id_2):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - data: Dictionary containing the data.\n",
        "    - stim_id_1: Stimulus ID for class 1.\n",
        "    - stim_id_2: Stimulus ID for class 2.\n",
        "\n",
        "    Returns:\n",
        "    - class_1_data: Data dictionary for class 1.\n",
        "    - class_2_data: Data dictionary for class 2.\n",
        "    \"\"\"\n",
        "    # Initialize dictionaries for the two classes\n",
        "    class_1_data = {'V': [], 't_on': [], 't_off': [], 'stim_id': []}\n",
        "    class_2_data = {'V': [], 't_on': [], 't_off': [], 'stim_id': []}\n",
        "\n",
        "    # Iterate through the stimulus IDs and split the data\n",
        "    for i, stim_id in enumerate(data['stim_id']):\n",
        "        t_on = data['t_on'][i]\n",
        "        t_off = data['t_off'][i]\n",
        "        if stim_id == stim_id_1:\n",
        "            class_1_data['V'].append(data['V'][t_on:t_off])\n",
        "            class_1_data['t_on'].append(t_on)\n",
        "            class_1_data['t_off'].append(t_off)\n",
        "            class_1_data['stim_id'].append(stim_id)\n",
        "        elif stim_id == stim_id_2:\n",
        "            class_2_data['V'].append(data['V'][t_on:t_off])\n",
        "            class_2_data['t_on'].append(t_on)\n",
        "            class_2_data['t_off'].append(t_off)\n",
        "            class_2_data['stim_id'].append(stim_id)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    class_1_data['V'] = np.array(class_1_data['V'])\n",
        "    class_1_data['t_on'] = np.array(class_1_data['t_on'])\n",
        "    class_1_data['t_off'] = np.array(class_1_data['t_off'])\n",
        "    class_1_data['stim_id'] = np.array(class_1_data['stim_id'])\n",
        "\n",
        "    class_2_data['V'] = np.array(class_2_data['V'])\n",
        "    class_2_data['t_on'] = np.array(class_2_data['t_on'])\n",
        "    class_2_data['t_off'] = np.array(class_2_data['t_off'])\n",
        "    class_2_data['stim_id'] = np.array(class_2_data['stim_id'])\n",
        "\n",
        "    return class_1_data, class_2_data\n",
        "\n",
        "stim_id_1 = 11\n",
        "stim_id_2 = 12\n",
        "\n",
        "class_1_data, class_2_data = split_classes(dat1, stim_id_1, stim_id_2)\n",
        "\n",
        "print(\"Class 1 Data Keys:\", class_1_data.keys())\n",
        "print(\"Class 2 Data Keys:\", class_2_data.keys())\n",
        "print(\"Class 1 V Shape:\", class_1_data['V'].shape)\n",
        "print(\"Class 2 V Shape:\", class_2_data['V'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z347WMf0FStm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vN2NyWmoorl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aklLd5vWFTR0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOM99WMy_HiR"
      },
      "source": [
        "# Dataset info #\n",
        "\n",
        "This is one of multiple ECoG datasets from Miller 2019, recorded in a clinical settings with a variety of tasks. Raw data and dataset paper are here:\n",
        "\n",
        "https://exhibits.stanford.edu/data/catalog/zk881ps0522\n",
        "https://www.nature.com/articles/s41562-019-0678-3\n",
        "\n",
        "This particular dataset was originally described in this paper:\n",
        "\n",
        "- Miller, Kai J., Gerwin Schalk, Eberhard E. Fetz, Marcel Den Nijs, Jeffrey G. Ojemann, and Rajesh PN Rao. \"Cortical activity during motor execution, motor imagery, and imagery-based online feedback.\" Proceedings of the National Academy of Sciences (2010): 200913697. doi: [10.1073/pnas.0913697107](https://doi.org/10.1073/pnas.0913697107)\n",
        "\n",
        "<br>\n",
        "\n",
        "`dat1` and `dat2` are data from the two blocks performed in each subject. The first one was the actual movements, the second one was motor imagery. For the movement task, from the original dataset instructions:\n",
        "\n",
        "*Patients performed simple, repetitive, motor tasks of hand (synchronous flexion and extension of all fingers, i.e., clenching and releasing a fist at a self-paced rate of ~1-2 Hz) or tongue (opening of mouth with protrusion and retraction of the tongue, i.e., sticking the tongue in and out, also at ~1-2 Hz). These movements were performed in an interval-based manner, alternating between movement and rest, and the side of move- ment was always contralateral to the side of cortical grid placement.*\n",
        "\n",
        "<br>\n",
        "\n",
        "For the imagery task, from the original dataset instructions:\n",
        "\n",
        "*Following the overt movement experiment, each subject performed an imagery task, imagining making identical movement rather than executing the movement. The imagery was kinesthetic rather than visual (“imagine yourself performing the actions like you just did”; i.e., “don’t imagine what it looked like, but imagine making the motions”).*\n",
        "\n",
        "<br>\n",
        "\n",
        "Sample rate is always 1000Hz, and the ECoG data has been notch-filtered at 60, 120, 180, 240 and 250Hz, followed by z-scoring across time and conversion to float16 to minimize size. Please convert back to float32 after loading the data in the notebook, to avoid unexpected behavior.\n",
        "\n",
        "Both experiments:\n",
        "* `dat['V']`: continuous voltage data (time by channels)\n",
        "* `dat['srate']`: acquisition rate (1000 Hz). All stimulus times are in units of this.  \n",
        "* `dat['t_on']`: time of stimulus onset in data samples\n",
        "* `dat['t_off']`: time of stimulus offset, always 400 samples after `t_on`\n",
        "* `dat['stim_id`]: identity of stimulus (11 = tongue, 12 = hand), real or imaginary stimulus\n",
        "* `dat['scale_uv']`: scale factor to multiply the data values to get to microvolts (uV).\n",
        "* `dat['locs`]`: 3D electrode positions on the brain surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA0IWh-U_HiS"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "from nimare import utils\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "locs = dat1['locs']\n",
        "view = plotting.view_markers(utils.tal2mni(locs),\n",
        "                             marker_labels=['%d'%k for k in np.arange(locs.shape[0])],\n",
        "                             marker_color='purple',\n",
        "                             marker_size=5)\n",
        "view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihqcUQpK_HiU"
      },
      "outputs": [],
      "source": [
        "# quick way to get broadband power in time-varying windows\n",
        "from scipy import signal\n",
        "\n",
        "# pick subject 0 and experiment 0 (real movements)\n",
        "dat1 = alldat[0][0]\n",
        "\n",
        "# V is the voltage data\n",
        "V = dat1['V'].astype('float32')\n",
        "\n",
        "# high-pass filter above 50 Hz\n",
        "b, a = signal.butter(3, [50], btype='high', fs=1000)\n",
        "V = signal.filtfilt(b, a, V, 0)\n",
        "\n",
        "# compute smooth envelope of this signal = approx power\n",
        "V = np.abs(V)**2\n",
        "b, a = signal.butter(3, [10], btype='low', fs=1000)\n",
        "V = signal.filtfilt(b, a, V, 0)\n",
        "\n",
        "# normalize each channel so its mean power is 1\n",
        "V = V/V.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCkiQFQN_HiU"
      },
      "outputs": [],
      "source": [
        "# average the broadband power across all tongue and hand trials\n",
        "nt, nchan = V.shape\n",
        "nstim = len(dat1['t_on'])\n",
        "\n",
        "trange = np.arange(0, 2000)\n",
        "ts = dat1['t_on'][:, np.newaxis] + trange\n",
        "V_epochs = np.reshape(V[ts, :], (nstim, 2000, nchan))\n",
        "\n",
        "V_tongue = (V_epochs[dat1['stim_id'] == 11]).mean(0)\n",
        "V_hand = (V_epochs[dat1['stim_id'] == 12]).mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyDV4Fz-_HiW"
      },
      "outputs": [],
      "source": [
        "# let's find the electrodes that distinguish tongue from hand movements\n",
        "# note the behaviors happen some time after the visual cue\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "for j in range(46):\n",
        "  ax = plt.subplot(5, 10, j+1)\n",
        "  plt.plot(trange, V_tongue[:, j])\n",
        "  plt.plot(trange, V_hand[:, j])\n",
        "  plt.title('ch%d'%j)\n",
        "  plt.xticks([0, 1000, 2000])\n",
        "  plt.ylim([0, 4])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XItZrWuU_HiY"
      },
      "outputs": [],
      "source": [
        "# let's look at all the trials for electrode 20 that has a good response to hand movements\n",
        "# we will sort trials by stimulus id\n",
        "plt.subplot(1, 3, 1)\n",
        "isort = np.argsort(dat1['stim_id'])\n",
        "plt.imshow(V_epochs[isort, :, 20].astype('float32'),\n",
        "           aspect='auto',\n",
        "           vmax=7, vmin=0,\n",
        "           cmap='magma')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVBZ2s56_HiZ"
      },
      "outputs": [],
      "source": [
        "# Electrode 42 seems to respond to tongue movements\n",
        "isort = np.argsort(dat1['stim_id'])\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(V_epochs[isort, :, 42].astype('float32'),\n",
        "           aspect='auto',\n",
        "           vmax=7, vmin=0,\n",
        "           cmap='magma')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
